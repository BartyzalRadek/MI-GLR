{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: TD methods\n",
    "\n",
    "Let's implement these methods for a simple classical problem (Cliffwalk) from the Sutton and Barto book. The code below can help you get started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First things first:** Spend some time getting familiar with the environment.\n",
    "\n",
    "    The board is a 4x12 matrix, indexed as 1D array:\n",
    "        0 = top leftt\n",
    "        11 = top right\n",
    "        12 = beginning of 2nd row from top at left side\n",
    "        ...\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \n",
    "    env.step(action) = (new_state, reward_of_this_state, done, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Episode 10 finished after 4733 timesteps with r=-51362.                 Avg score: -51953.8\n",
      "INFO: Episode 20 finished after 73 timesteps with r=-568.                 Avg score: -27855.3\n",
      "INFO: Episode 30 finished after 31 timesteps with r=-31.                 Avg score: -18675.533333333333\n",
      "INFO: Episode 40 finished after 18 timesteps with r=-18.                 Avg score: -14038.975\n",
      "INFO: Episode 50 finished after 32 timesteps with r=-230.                 Avg score: -11256.06\n",
      "INFO: Episode 60 finished after 15 timesteps with r=-15.                 Avg score: -9402.033333333333\n",
      "INFO: Episode 70 finished after 41 timesteps with r=-338.                 Avg score: -8066.3\n",
      "INFO: Episode 80 finished after 23 timesteps with r=-23.                 Avg score: -7070.625\n",
      "INFO: Episode 90 finished after 28 timesteps with r=-127.                 Avg score: -6289.533333333334\n",
      "INFO: Episode 100 finished after 28 timesteps with r=-28.                 Avg score: -5665.83\n",
      "INFO: Episode 110 finished after 22 timesteps with r=-22.                 Avg score: -5153.536363636364\n",
      "INFO: Episode 120 finished after 42 timesteps with r=-141.                 Avg score: -4727.75\n",
      "INFO: Episode 130 finished after 17 timesteps with r=-17.                 Avg score: -4365.6\n",
      "INFO: Episode 140 finished after 13 timesteps with r=-13.                 Avg score: -4055.9785714285713\n",
      "INFO: Episode 150 finished after 16 timesteps with r=-16.                 Avg score: -3787.5866666666666\n",
      "INFO: Episode 160 finished after 21 timesteps with r=-21.                 Avg score: -3552.09375\n",
      "INFO: Episode 170 finished after 15 timesteps with r=-15.                 Avg score: -3344.258823529412\n",
      "INFO: Episode 180 finished after 15 timesteps with r=-15.                 Avg score: -3159.438888888889\n",
      "INFO: Episode 190 finished after 27 timesteps with r=-126.                 Avg score: -2994.642105263158\n",
      "INFO: Episode 200 finished after 17 timesteps with r=-17.                 Avg score: -2845.855\n",
      "INFO: Episode 210 finished after 19 timesteps with r=-19.                 Avg score: -2712.285714285714\n",
      "INFO: Episode 220 finished after 13 timesteps with r=-13.                 Avg score: -2590.7863636363636\n",
      "INFO: Episode 230 finished after 13 timesteps with r=-13.                 Avg score: -2479.865217391304\n",
      "INFO: Episode 240 finished after 21 timesteps with r=-21.                 Avg score: -2377.2166666666667\n",
      "INFO: Episode 250 finished after 19 timesteps with r=-118.                 Avg score: -2283.624\n",
      "INFO: Episode 260 finished after 14 timesteps with r=-14.                 Avg score: -2198.023076923077\n",
      "INFO: Episode 270 finished after 14 timesteps with r=-14.                 Avg score: -2117.5962962962963\n",
      "INFO: Episode 280 finished after 13 timesteps with r=-13.                 Avg score: -2042.4607142857142\n",
      "INFO: Episode 290 finished after 13 timesteps with r=-13.                 Avg score: -1972.8862068965518\n",
      "INFO: Episode 300 finished after 19 timesteps with r=-19.                 Avg score: -1907.96\n",
      "INFO: Episode 310 finished after 13 timesteps with r=-13.                 Avg score: -1847.1806451612904\n",
      "INFO: Episode 320 finished after 13 timesteps with r=-13.                 Avg score: -1789.871875\n",
      "INFO: Episode 330 finished after 14 timesteps with r=-113.                 Avg score: -1736.3545454545454\n",
      "INFO: Episode 340 finished after 13 timesteps with r=-13.                 Avg score: -1685.6735294117648\n",
      "INFO: Episode 350 finished after 13 timesteps with r=-13.                 Avg score: -1638.5114285714285\n",
      "INFO: Episode 360 finished after 25 timesteps with r=-124.                 Avg score: -1593.6944444444443\n",
      "INFO: Episode 370 finished after 13 timesteps with r=-13.                 Avg score: -1550.9756756756756\n",
      "INFO: Episode 380 finished after 17 timesteps with r=-116.                 Avg score: -1511.3315789473684\n",
      "INFO: Episode 390 finished after 13 timesteps with r=-13.                 Avg score: -1472.9179487179488\n",
      "INFO: Episode 400 finished after 13 timesteps with r=-13.                 Avg score: -1436.9375\n",
      "INFO: Episode 410 finished after 13 timesteps with r=-13.                 Avg score: -1402.4658536585366\n",
      "INFO: Episode 420 finished after 13 timesteps with r=-13.                 Avg score: -1369.3880952380953\n",
      "INFO: Episode 430 finished after 13 timesteps with r=-13.                 Avg score: -1338.0790697674418\n",
      "INFO: Episode 440 finished after 13 timesteps with r=-13.                 Avg score: -1307.9681818181818\n",
      "INFO: Episode 450 finished after 21 timesteps with r=-120.                 Avg score: -1279.4377777777777\n",
      "INFO: Episode 460 finished after 13 timesteps with r=-13.                 Avg score: -1252.128260869565\n",
      "INFO: Episode 470 finished after 13 timesteps with r=-13.                 Avg score: -1225.995744680851\n",
      "INFO: Episode 480 finished after 13 timesteps with r=-13.                 Avg score: -1200.7270833333334\n",
      "INFO: Episode 490 finished after 13 timesteps with r=-13.                 Avg score: -1176.4897959183672\n",
      "INFO: Episode 500 finished after 17 timesteps with r=-116.                 Avg score: -1153.426\n",
      "INFO: Episode 510 finished after 13 timesteps with r=-13.                 Avg score: -1131.0960784313725\n",
      "INFO: Episode 520 finished after 13 timesteps with r=-13.                 Avg score: -1109.5942307692308\n",
      "INFO: Episode 530 finished after 13 timesteps with r=-13.                 Avg score: -1089.3075471698114\n",
      "INFO: Episode 540 finished after 15 timesteps with r=-15.                 Avg score: -1069.387037037037\n",
      "INFO: Episode 550 finished after 13 timesteps with r=-13.                 Avg score: -1050.2072727272728\n",
      "INFO: Episode 560 finished after 15 timesteps with r=-15.                 Avg score: -1031.892857142857\n",
      "INFO: Episode 570 finished after 13 timesteps with r=-13.                 Avg score: -1014.021052631579\n",
      "INFO: Episode 580 finished after 13 timesteps with r=-13.                 Avg score: -996.7758620689655\n",
      "INFO: Episode 590 finished after 13 timesteps with r=-13.                 Avg score: -980.1016949152543\n",
      "INFO: Episode 600 finished after 13 timesteps with r=-13.                 Avg score: -964.16\n",
      "INFO: Episode 610 finished after 13 timesteps with r=-13.                 Avg score: -948.5704918032787\n",
      "INFO: Episode 620 finished after 13 timesteps with r=-13.                 Avg score: -933.6451612903226\n",
      "INFO: Episode 630 finished after 13 timesteps with r=-13.                 Avg score: -919.036507936508\n",
      "INFO: Episode 640 finished after 15 timesteps with r=-15.                 Avg score: -905.0515625\n",
      "INFO: Episode 650 finished after 13 timesteps with r=-13.                 Avg score: -891.3292307692308\n",
      "INFO: Episode 660 finished after 13 timesteps with r=-13.                 Avg score: -878.0212121212121\n",
      "INFO: Episode 670 finished after 13 timesteps with r=-13.                 Avg score: -865.1134328358208\n",
      "INFO: Episode 680 finished after 13 timesteps with r=-13.                 Avg score: -852.7426470588235\n",
      "INFO: Episode 690 finished after 13 timesteps with r=-13.                 Avg score: -840.5753623188406\n",
      "INFO: Episode 700 finished after 23 timesteps with r=-122.                 Avg score: -829.3771428571429\n",
      "INFO: Episode 710 finished after 13 timesteps with r=-13.                 Avg score: -817.8816901408451\n",
      "INFO: Episode 720 finished after 13 timesteps with r=-13.                 Avg score: -806.7083333333334\n",
      "INFO: Episode 730 finished after 15 timesteps with r=-15.                 Avg score: -796.1479452054795\n",
      "INFO: Episode 740 finished after 13 timesteps with r=-13.                 Avg score: -785.5702702702703\n",
      "INFO: Episode 750 finished after 13 timesteps with r=-13.                 Avg score: -775.2773333333333\n",
      "INFO: Episode 760 finished after 13 timesteps with r=-13.                 Avg score: -765.2473684210527\n",
      "INFO: Episode 770 finished after 13 timesteps with r=-13.                 Avg score: -755.6207792207792\n",
      "INFO: Episode 780 finished after 13 timesteps with r=-13.                 Avg score: -746.1\n",
      "INFO: Episode 790 finished after 15 timesteps with r=-15.                 Avg score: -736.8253164556962\n",
      "INFO: Episode 800 finished after 13 timesteps with r=-13.                 Avg score: -727.915\n",
      "INFO: Episode 810 finished after 13 timesteps with r=-13.                 Avg score: -719.0888888888888\n",
      "INFO: Episode 820 finished after 13 timesteps with r=-13.                 Avg score: -710.4804878048781\n",
      "INFO: Episode 830 finished after 13 timesteps with r=-13.                 Avg score: -702.0771084337349\n",
      "INFO: Episode 840 finished after 13 timesteps with r=-13.                 Avg score: -693.875\n",
      "INFO: Episode 850 finished after 13 timesteps with r=-13.                 Avg score: -685.8670588235294\n",
      "INFO: Episode 860 finished after 23 timesteps with r=-122.                 Avg score: -678.1697674418605\n",
      "INFO: Episode 870 finished after 13 timesteps with r=-13.                 Avg score: -670.6448275862069\n",
      "INFO: Episode 880 finished after 13 timesteps with r=-13.                 Avg score: -663.1715909090909\n",
      "INFO: Episode 890 finished after 13 timesteps with r=-13.                 Avg score: -655.870786516854\n",
      "INFO: Episode 900 finished after 13 timesteps with r=-13.                 Avg score: -648.73\n",
      "INFO: Episode 910 finished after 13 timesteps with r=-13.                 Avg score: -641.7527472527472\n",
      "INFO: Episode 920 finished after 13 timesteps with r=-13.                 Avg score: -634.9228260869565\n",
      "INFO: Episode 930 finished after 13 timesteps with r=-13.                 Avg score: -628.3537634408602\n",
      "INFO: Episode 940 finished after 13 timesteps with r=-13.                 Avg score: -621.8074468085107\n",
      "INFO: Episode 950 finished after 13 timesteps with r=-13.                 Avg score: -615.3989473684211\n",
      "INFO: Episode 960 finished after 13 timesteps with r=-13.                 Avg score: -609.1260416666667\n",
      "INFO: Episode 970 finished after 13 timesteps with r=-13.                 Avg score: -602.9824742268041\n",
      "INFO: Episode 980 finished after 13 timesteps with r=-13.                 Avg score: -596.9622448979592\n",
      "INFO: Episode 990 finished after 13 timesteps with r=-13.                 Avg score: -591.0656565656566\n",
      "INFO: Episode 1000 finished after 13 timesteps with r=-13.                 Avg score: -585.285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "def sarsa_update(Q, state, action, reward, new_state, new_action):\n",
    "    Q[state, action] = Q[state, action] + alpha*(reward + gamma*Q[new_state, new_action] - Q[state, action])\n",
    "    return Q\n",
    "\n",
    "def Q_learning_update(Q, state, action, reward, new_state, new_action):\n",
    "    Q[state, action] = Q[state, action] + alpha*(reward + gamma*np.max(Q[new_state, :]) - Q[state, action])\n",
    "    return Q\n",
    "\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "gamma = 0.99 \n",
    "alpha = 0.1 # learnintg rate\n",
    "n_episodes = 1000\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Play randomly 10 episodes, then reduce slowly the randomness\n",
    "    policy = epsilon_greedy_policy(Q, epsilon=10./(j+1), actions = actions ) \n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    t=0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        action = policy(state)    \n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if state == new_state:\n",
    "            reward -= 1\n",
    "        \n",
    "        Q = Q_learning_update(Q, state, action, reward, new_state, new_action)\n",
    "            \n",
    "            \n",
    "        state, action = new_state, new_action\n",
    "            \n",
    "        if done:\n",
    "            score.append(total_reward)\n",
    "            \n",
    "            if (j+1)%10 == 0:\n",
    "                print(\"INFO: Episode {} finished after {} timesteps with r={}. \\\n",
    "                Avg score: {}\".format(j+1, t, total_reward, np.mean(score)))\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control question**: Which trajectories are found by which algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory found by SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simulate_best():\n",
    "    policy = epsilon_greedy_policy(Q, epsilon=0, actions = actions )\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and t < 20:\n",
    "        t+= 1\n",
    "        action = policy(state)    \n",
    "        state, reward, done, _ =  env.step(action)\n",
    "        env.render()\n",
    "\n",
    "simulate_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory found by Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simulate_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
