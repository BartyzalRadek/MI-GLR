{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: TD methods\n",
    "\n",
    "Let's implement these methods for a simple classical problem (Cliffwalk) from the Sutton and Barto book. The code below can help you get started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First things first:** Spend some time getting familiar with the environment.\n",
    "\n",
    "    The board is a 4x12 matrix, indexed as 1D array:\n",
    "        0 = top leftt\n",
    "        11 = top right\n",
    "        12 = beginning of 2nd row from top at left side\n",
    "        ...\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \n",
    "    env.step(action) = (new_state, reward_of_this_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Episode 10 finished after 6222 timesteps with r=-66018.                 Avg score: -62974.1\n",
      "INFO: Episode 20 finished after 55 timesteps with r=-55.                 Avg score: -31991.65\n",
      "INFO: Episode 30 finished after 26 timesteps with r=-26.                 Avg score: -21352.7\n",
      "INFO: Episode 40 finished after 19 timesteps with r=-19.                 Avg score: -16026.1\n",
      "INFO: Episode 50 finished after 18 timesteps with r=-18.                 Avg score: -12827.0\n",
      "INFO: Episode 60 finished after 19 timesteps with r=-19.                 Avg score: -10692.766666666666\n",
      "INFO: Episode 70 finished after 18 timesteps with r=-18.                 Avg score: -9168.0\n",
      "INFO: Episode 80 finished after 20 timesteps with r=-20.                 Avg score: -8024.375\n",
      "INFO: Episode 90 finished after 19 timesteps with r=-19.                 Avg score: -7134.9111111111115\n",
      "INFO: Episode 100 finished after 17 timesteps with r=-17.                 Avg score: -6423.26\n",
      "INFO: Episode 110 finished after 18 timesteps with r=-18.                 Avg score: -5841.0\n",
      "INFO: Episode 120 finished after 17 timesteps with r=-17.                 Avg score: -5355.775\n",
      "INFO: Episode 130 finished after 20 timesteps with r=-20.                 Avg score: -4945.2\n",
      "INFO: Episode 140 finished after 20 timesteps with r=-20.                 Avg score: -4593.307142857143\n",
      "INFO: Episode 150 finished after 19 timesteps with r=-19.                 Avg score: -4288.293333333333\n",
      "INFO: Episode 160 finished after 19 timesteps with r=-19.                 Avg score: -4022.03125\n",
      "INFO: Episode 170 finished after 23 timesteps with r=-23.                 Avg score: -3786.541176470588\n",
      "INFO: Episode 180 finished after 19 timesteps with r=-19.                 Avg score: -3577.177777777778\n",
      "INFO: Episode 190 finished after 17 timesteps with r=-17.                 Avg score: -3389.9\n",
      "INFO: Episode 200 finished after 21 timesteps with r=-21.                 Avg score: -3221.84\n",
      "INFO: Episode 210 finished after 17 timesteps with r=-17.                 Avg score: -3069.2428571428572\n",
      "INFO: Episode 220 finished after 18 timesteps with r=-18.                 Avg score: -2930.559090909091\n",
      "INFO: Episode 230 finished after 17 timesteps with r=-17.                 Avg score: -2803.9086956521737\n",
      "INFO: Episode 240 finished after 17 timesteps with r=-17.                 Avg score: -2687.804166666667\n",
      "INFO: Episode 250 finished after 17 timesteps with r=-17.                 Avg score: -2581.016\n",
      "INFO: Episode 260 finished after 18 timesteps with r=-18.                 Avg score: -2482.4192307692306\n",
      "INFO: Episode 270 finished after 17 timesteps with r=-17.                 Avg score: -2391.137037037037\n",
      "INFO: Episode 280 finished after 19 timesteps with r=-19.                 Avg score: -2307.214285714286\n",
      "INFO: Episode 290 finished after 19 timesteps with r=-19.                 Avg score: -2228.2620689655173\n",
      "INFO: Episode 300 finished after 17 timesteps with r=-17.                 Avg score: -2154.596666666667\n",
      "INFO: Episode 310 finished after 17 timesteps with r=-17.                 Avg score: -2085.658064516129\n",
      "INFO: Episode 320 finished after 17 timesteps with r=-17.                 Avg score: -2021.028125\n",
      "INFO: Episode 330 finished after 21 timesteps with r=-21.                 Avg score: -1960.3242424242424\n",
      "INFO: Episode 340 finished after 17 timesteps with r=-17.                 Avg score: -1903.1676470588236\n",
      "INFO: Episode 350 finished after 17 timesteps with r=-17.                 Avg score: -1849.32\n",
      "INFO: Episode 360 finished after 17 timesteps with r=-17.                 Avg score: -1798.4305555555557\n",
      "INFO: Episode 370 finished after 19 timesteps with r=-19.                 Avg score: -1750.2972972972973\n",
      "INFO: Episode 380 finished after 17 timesteps with r=-17.                 Avg score: -1704.7\n",
      "INFO: Episode 390 finished after 17 timesteps with r=-17.                 Avg score: -1661.428205128205\n",
      "INFO: Episode 400 finished after 17 timesteps with r=-17.                 Avg score: -1620.3325\n",
      "INFO: Episode 410 finished after 17 timesteps with r=-17.                 Avg score: -1581.2365853658537\n",
      "INFO: Episode 420 finished after 17 timesteps with r=-17.                 Avg score: -1544.0047619047618\n",
      "INFO: Episode 430 finished after 17 timesteps with r=-17.                 Avg score: -1508.5046511627907\n",
      "INFO: Episode 440 finished after 17 timesteps with r=-17.                 Avg score: -1474.6272727272728\n",
      "INFO: Episode 450 finished after 17 timesteps with r=-17.                 Avg score: -1442.24\n",
      "INFO: Episode 460 finished after 19 timesteps with r=-19.                 Avg score: -1411.2695652173913\n",
      "INFO: Episode 470 finished after 17 timesteps with r=-17.                 Avg score: -1381.608510638298\n",
      "INFO: Episode 480 finished after 17 timesteps with r=-17.                 Avg score: -1353.18125\n",
      "INFO: Episode 490 finished after 17 timesteps with r=-17.                 Avg score: -1325.9142857142858\n",
      "INFO: Episode 500 finished after 17 timesteps with r=-17.                 Avg score: -1299.946\n",
      "INFO: Episode 510 finished after 17 timesteps with r=-17.                 Avg score: -1274.792156862745\n",
      "INFO: Episode 520 finished after 17 timesteps with r=-17.                 Avg score: -1250.6153846153845\n",
      "INFO: Episode 530 finished after 17 timesteps with r=-17.                 Avg score: -1227.345283018868\n",
      "INFO: Episode 540 finished after 19 timesteps with r=-19.                 Avg score: -1204.9407407407407\n",
      "INFO: Episode 550 finished after 17 timesteps with r=-17.                 Avg score: -1183.3490909090908\n",
      "INFO: Episode 560 finished after 17 timesteps with r=-17.                 Avg score: -1162.8785714285714\n",
      "INFO: Episode 570 finished after 17 timesteps with r=-17.                 Avg score: -1142.7771929824562\n",
      "INFO: Episode 580 finished after 17 timesteps with r=-17.                 Avg score: -1123.3689655172413\n",
      "INFO: Episode 590 finished after 17 timesteps with r=-17.                 Avg score: -1104.6169491525425\n",
      "INFO: Episode 600 finished after 17 timesteps with r=-17.                 Avg score: -1086.4933333333333\n",
      "INFO: Episode 610 finished after 18 timesteps with r=-18.                 Avg score: -1068.967213114754\n",
      "INFO: Episode 620 finished after 17 timesteps with r=-17.                 Avg score: -1052.0\n",
      "INFO: Episode 630 finished after 17 timesteps with r=-17.                 Avg score: -1035.5777777777778\n",
      "INFO: Episode 640 finished after 19 timesteps with r=-19.                 Avg score: -1019.6703125\n",
      "INFO: Episode 650 finished after 17 timesteps with r=-17.                 Avg score: -1004.2446153846154\n",
      "INFO: Episode 660 finished after 17 timesteps with r=-17.                 Avg score: -989.2863636363636\n",
      "INFO: Episode 670 finished after 17 timesteps with r=-17.                 Avg score: -974.7746268656716\n",
      "INFO: Episode 680 finished after 17 timesteps with r=-17.                 Avg score: -960.689705882353\n",
      "INFO: Episode 690 finished after 17 timesteps with r=-17.                 Avg score: -947.0159420289855\n",
      "INFO: Episode 700 finished after 18 timesteps with r=-18.                 Avg score: -933.7414285714286\n",
      "INFO: Episode 710 finished after 17 timesteps with r=-17.                 Avg score: -920.8352112676056\n",
      "INFO: Episode 720 finished after 17 timesteps with r=-17.                 Avg score: -908.2833333333333\n",
      "INFO: Episode 730 finished after 17 timesteps with r=-17.                 Avg score: -896.0780821917808\n",
      "INFO: Episode 740 finished after 17 timesteps with r=-17.                 Avg score: -884.2054054054054\n",
      "INFO: Episode 750 finished after 17 timesteps with r=-17.                 Avg score: -872.6493333333333\n",
      "INFO: Episode 760 finished after 17 timesteps with r=-17.                 Avg score: -861.3934210526315\n",
      "INFO: Episode 770 finished after 17 timesteps with r=-17.                 Avg score: -850.4324675324675\n",
      "INFO: Episode 780 finished after 17 timesteps with r=-17.                 Avg score: -839.748717948718\n",
      "INFO: Episode 790 finished after 17 timesteps with r=-17.                 Avg score: -829.3341772151899\n",
      "INFO: Episode 800 finished after 17 timesteps with r=-17.                 Avg score: -819.1825\n",
      "INFO: Episode 810 finished after 17 timesteps with r=-17.                 Avg score: -809.2827160493828\n",
      "INFO: Episode 820 finished after 18 timesteps with r=-18.                 Avg score: -799.6243902439024\n",
      "INFO: Episode 830 finished after 17 timesteps with r=-17.                 Avg score: -790.2\n",
      "INFO: Episode 840 finished after 17 timesteps with r=-17.                 Avg score: -780.995238095238\n",
      "INFO: Episode 850 finished after 17 timesteps with r=-17.                 Avg score: -772.0117647058823\n",
      "INFO: Episode 860 finished after 17 timesteps with r=-17.                 Avg score: -763.2325581395348\n",
      "INFO: Episode 870 finished after 17 timesteps with r=-17.                 Avg score: -754.6574712643678\n",
      "INFO: Episode 880 finished after 17 timesteps with r=-17.                 Avg score: -746.2761363636364\n",
      "INFO: Episode 890 finished after 18 timesteps with r=-18.                 Avg score: -738.1977528089888\n",
      "INFO: Episode 900 finished after 19 timesteps with r=-19.                 Avg score: -730.1933333333334\n",
      "INFO: Episode 910 finished after 17 timesteps with r=-17.                 Avg score: -722.3582417582418\n",
      "INFO: Episode 920 finished after 19 timesteps with r=-19.                 Avg score: -714.6978260869565\n",
      "INFO: Episode 930 finished after 17 timesteps with r=-17.                 Avg score: -707.1978494623656\n",
      "INFO: Episode 940 finished after 17 timesteps with r=-17.                 Avg score: -699.8553191489361\n",
      "INFO: Episode 950 finished after 17 timesteps with r=-17.                 Avg score: -692.6673684210526\n",
      "INFO: Episode 960 finished after 17 timesteps with r=-17.                 Avg score: -685.6322916666667\n",
      "INFO: Episode 970 finished after 17 timesteps with r=-17.                 Avg score: -678.740206185567\n",
      "INFO: Episode 980 finished after 17 timesteps with r=-17.                 Avg score: -672.0908163265306\n",
      "INFO: Episode 990 finished after 17 timesteps with r=-17.                 Avg score: -665.4777777777778\n",
      "INFO: Episode 1000 finished after 17 timesteps with r=-17.                 Avg score: -658.993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon, actions):\n",
    "    \"\"\" Q is a numpy array, epsilon between 0,1 \n",
    "    and a list of actions\"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        if np.random.rand()>epsilon:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = np.random.choice(actions)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "def sarsa_update(Q, state, action, reward, new_state, new_action):\n",
    "    Q[state, action] = Q[state, action] + alpha*(reward + gamma*Q[new_state, new_action] - Q[state, action])\n",
    "    return Q\n",
    "\n",
    "def Q_learning_update(Q, state, action, reward, new_state, new_action):\n",
    "    Q[state, action] = Q[state, action] + alpha*(reward + gamma*np.max(Q[new_state, :]) - Q[state, action])\n",
    "    return Q\n",
    "\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "gamma = 0.99 \n",
    "alpha = 0.1 # learnintg rate\n",
    "n_episodes = 1000\n",
    "\n",
    "\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "score = []    \n",
    "for j in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Play randomly 10 episodes, then reduce slowly the randomness\n",
    "    policy = epsilon_greedy_policy(Q, epsilon=10./(j+1), actions = actions ) \n",
    "    \n",
    "    \n",
    "    ### Generate sample episode\n",
    "    t=0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        action = policy(state)    \n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        new_action = policy(new_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        #Book-keeping\n",
    "        #if done:\n",
    "           \n",
    "            \n",
    "        #else:\n",
    "        Q = sarsa_update(Q, state, action, reward, new_state, new_action)\n",
    "            \n",
    "            \n",
    "        state, action = new_state, new_action\n",
    "            \n",
    "        if done:\n",
    "            score.append(total_reward)\n",
    "            \n",
    "            if (j+1)%10 == 0:\n",
    "                print(\"INFO: Episode {} finished after {} timesteps with r={}. \\\n",
    "                Avg score: {}\".format(j+1, t, total_reward, np.mean(score)))\n",
    "            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control question**: Which trajectories are found by which algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simulate_best():\n",
    "    policy = epsilon_greedy_policy(Q, epsilon=0, actions = actions )\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    done = False\n",
    "    while not done and t < 20:\n",
    "        t+= 1\n",
    "        action = policy(state)    \n",
    "        new_state, reward, done, _ =  env.step(action)\n",
    "        env.render()\n",
    "\n",
    "simulate_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
