{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Policy methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task**\n",
    "\n",
    "- Compare the performance of REINFORCE and REINFORCE with baseline for `CliffWalking`. You can use some of the code of tutorial 6 as inspiration. (4p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First things first:** Spend some time getting familiar with the environment.\n",
    "\n",
    "    The board is a 4x12 matrix, indexed as 1D array:\n",
    "        0 = top leftt\n",
    "        11 = top right\n",
    "        12 = beginning of 2nd row from top at left side\n",
    "        ...\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \n",
    "    env.step(action) = (new_state, reward_of_this_state, done, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  48\n",
      "Number of actions:  4\n",
      "Result of env.step(0):  (24, -1, False, {'prob': 1.0})\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "print(\"Number of states: \", env.observation_space.n)\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Result of env.step(0): \", env.step(0))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(avg_rewards, title):\n",
    "    plt.plot(np.array(range(len(avg_rewards)))*100, avg_rewards)\n",
    "    plt.ylabel('average reward in 100 episodes')\n",
    "    #plt.ylim(0.2, 1)\n",
    "    plt.xlabel('episode')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_x = np.max(x)\n",
    "    return np.exp(x - max_x) / np.sum(np.exp(x - max_x), axis=0)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self,env, alpha=1e-6, gamma=1, debug=True):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.debug = debug\n",
    "        self.env = env\n",
    "        \n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_states = env.observation_space.n\n",
    "        \n",
    "        self.possible_actions = range(env.action_space.n)\n",
    "        self.state_dim = len(self.featurize(env.reset(), 0))\n",
    "        \n",
    "        self.w = np.ones(shape=(self.state_dim,), dtype=float) #parameters of the function approximating probabilities of actions\n",
    "        \n",
    "        self.last_action_probabilities = np.ones(shape=(self.n_actions,), dtype=float) / self.n_actions\n",
    "        self.last_action_probabilities = np.tile(self.last_action_probabilities,(self.n_states,1))\n",
    "    \n",
    "    def featurize(self,state, action):\n",
    "        \"\"\"\n",
    "        Turn state + action into feature vector so it can be inputted into the policy function.\n",
    "        \"\"\"\n",
    "        s = np.zeros(shape=(self.n_states+self.n_actions,), dtype=float)\n",
    "        s[state] = 1\n",
    "        s[self.n_states + action] = 1\n",
    "        return s\n",
    "    \n",
    "    def all_features(self, state):\n",
    "        \"\"\"\n",
    "        Return features for all actions and a given state.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        s = np.zeros(shape=(self.n_actions, self.n_states+self.n_actions), dtype=float)\n",
    "        for a in self.possible_actions:\n",
    "            s[a, state] = 1\n",
    "            s[a, self.n_states + a] = 1\n",
    "            \n",
    "        print(\"Correct action features:\\n\",s)\n",
    "        \"\"\"\n",
    "        \n",
    "        s = np.zeros(shape=(self.n_actions, self.n_states+self.n_actions), dtype=float)\n",
    "        s[:,state] = 1\n",
    "        s[:, self.n_states:] = np.identity(self.n_actions)\n",
    "        \n",
    "        #print(\"Faster action features:\\n\",s)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def get_action_probabilities(self, state):\n",
    "        \"\"\"\n",
    "        # Slower way of calculation\n",
    "        h = [] \n",
    "        for a in self.possible_actions:    # using for cycle to improve readability\n",
    "            sa = self.featurize(state, a)\n",
    "            h.append(np.dot(sa, self.w).flatten()) # linear function of numerical prefferences of all actions\n",
    "        h = np.array(h).flatten()\n",
    "        \"\"\"\n",
    "        h = self.all_features(state).dot(self.w.reshape(-1,1)).flatten()\n",
    "        soft = softmax(h)\n",
    "        if self.debug:\n",
    "            print(\"state: {} h: {}\".format(state, h))\n",
    "            print(\"action_probabilities: \", soft)\n",
    "        return soft\n",
    "    \n",
    "    def gradient(self, state, action):\n",
    "        \"\"\"\n",
    "        See Exercise 13.3 in Barton, Sutton book.\n",
    "        \"\"\"\n",
    "        x_sa = self.featurize(state, action)\n",
    "        action_probabilities = self.get_action_probabilities(state)\n",
    "        \n",
    "        sum_of_action_prob_times_x_sb = np.zeros(shape=x_sa.shape)\n",
    "        for a,p_a in enumerate(action_probabilities):\n",
    "            sum_of_action_prob_times_x_sb += p_a * self.featurize(state, a) # this sums to 1 on the position of state\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"x_sa=\\n\", x_sa)\n",
    "            print(\"sum_of_action_prob_times_x_sb=\\n\", sum_of_action_prob_times_x_sb)\n",
    "        return x_sa - sum_of_action_prob_times_x_sb\n",
    "    \n",
    "    def update(self, episode, max_steps=200):\n",
    "        \"\"\"\n",
    "        Update the parameters by gradient descend.\n",
    "        \n",
    "        episode = list of [state, action, reward] over the whole episode\n",
    "        \"\"\"\n",
    "        episode = np.array(episode)\n",
    "        \n",
    "        G = []\n",
    "        for [state, action, reward] in episode[::-1]: # calculate cumulative rewwards till the end from each step\n",
    "            if len(G) == 0:\n",
    "                G.append(reward)\n",
    "            else:\n",
    "                G.append(reward + G[-1])\n",
    "        G = G[::-1]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Episode shape={} whole:\\n{}\".format(episode.shape, episode))\n",
    "            print(\"G: \", G)\n",
    "\n",
    "        # Update parameters after each step - BUT choose only N random steps otherwise it will run forever.\n",
    "        if len(episode) <= max_steps:\n",
    "            chosen_steps = episode\n",
    "        else:\n",
    "            chosen_step_indxs = np.random.choice(len(episode), max_steps)\n",
    "            chosen_steps = episode[chosen_step_indxs]\n",
    "        \n",
    "        for t,[state, action, reward] in enumerate(chosen_steps):\n",
    "            gradient = self.gradient(state, action)\n",
    "            self.w += self.alpha * (np.power(self.gamma,t)) * G[t] * gradient \n",
    "            if self.debug:\n",
    "                print(\"  t={}  Gradient={}  G[t]={} new_w={}\".format(t, gradient, G[t], self.w))\n",
    "            \n",
    "        \n",
    "        for state in range(self.n_states):\n",
    "            self.last_action_probabilities[state] = self.get_action_probabilities(state)\n",
    "        \n",
    "    def choose_action(self,state):\n",
    "        \"\"\"\n",
    "        Calculate numerical prefferences of every action in a given state = h(s,a,w)\n",
    "        Policy will then calculate action probabilities and sample an action according to them.\n",
    "        \"\"\"\n",
    "        action_probabilities = self.last_action_probabilities[state] # policy is not updated while playing\n",
    "        return np.random.choice(self.possible_actions, 1, p=action_probabilities)[0]\n",
    "\n",
    "    \n",
    "def run_episodes(n_episodes = 1000, gamma = 1, max_update_steps=200, debug=True):\n",
    "    env = gym.make('CliffWalking-v0')\n",
    "\n",
    "    policy = Policy(env, gamma=gamma, debug=debug)\n",
    "    episode_rewards = []\n",
    "    avg_episode_rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        episode = []\n",
    "\n",
    "        while not done:\n",
    "            action = policy.choose_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            #print(\"State: {} Action: {} Reward: {}\".format(state, action, reward))\n",
    "            ep_reward += reward\n",
    "\n",
    "            # Keep track of the states    \n",
    "            episode.append([state, action, reward])\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            if debug:\n",
    "                if reward < -1:\n",
    "                    break\n",
    "                    \n",
    "            if reward < -1:\n",
    "                episode = []\n",
    "        \n",
    "        # Update policy\n",
    "        policy.update(episode, max_steps=max_update_steps)\n",
    "        #print(\"done\")\n",
    "\n",
    "        episode_rewards.append(ep_reward)\n",
    "        # Show stats\n",
    "        if (ep) % 10 == 0:\n",
    "            avg_episode_rewards.append(np.average(episode_rewards[-100:]))\n",
    "            print(\"Reward at episode {} is {} | avg in last 100: {}\"\n",
    "                  .format(ep,ep_reward, avg_episode_rewards[-1]))\n",
    "            \n",
    "    env.close()\n",
    "    return episode_rewards, avg_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward at episode 0 is -221296 | avg in last 100: -221296.0\n",
      "Reward at episode 10 is -21974 | avg in last 100: -87838.72727272728\n",
      "Reward at episode 20 is -161335 | avg in last 100: -89103.57142857143\n",
      "Reward at episode 30 is -77345 | avg in last 100: -84050.6129032258\n",
      "Reward at episode 40 is -116444 | avg in last 100: -73705.0\n",
      "Reward at episode 50 is -74939 | avg in last 100: -72209.11764705883\n",
      "Reward at episode 60 is -6243 | avg in last 100: -78845.55737704918\n",
      "Reward at episode 70 is -194546 | avg in last 100: -83737.45070422535\n",
      "Reward at episode 80 is -7057 | avg in last 100: -81188.56790123456\n",
      "Reward at episode 90 is -107477 | avg in last 100: -77632.73626373627\n",
      "Reward at episode 100 is -41177 | avg in last 100: -76572.24\n",
      "Reward at episode 110 is -12141 | avg in last 100: -76346.51\n",
      "Reward at episode 120 is -75800 | avg in last 100: -74113.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-cc8fd829f600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepisode_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_episode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_update_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-c72c38d36e5b>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(n_episodes, gamma, max_update_steps, debug)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[1;31m#print(\"State: {} Action: {} Reward: {}\".format(state, action, reward))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards, avg_episode_rewards = run_episodes(n_episodes = 1000, gamma = 1, max_update_steps=2000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13, 14, 15],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [ 0,  1,  2,  3],\n",
       "       [ 0,  1,  2,  3]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(range(20)).reshape(5,4)\n",
    "x[np.random.choice(len(x), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
